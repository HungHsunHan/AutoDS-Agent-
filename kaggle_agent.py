import io
import json
import logging
import os
import re
import shutil
import sys
from contextlib import redirect_stderr, redirect_stdout
from typing import Annotated, Dict, List, TypedDict

import pandas as pd
from dotenv import load_dotenv
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph
from langsmith import traceable

# Import prompt definitions
from prompts import (
    DATA_ANALYST_PROMPT,
    FEATURE_ENGINEER_PROMPT,
    MODEL_ARCHITECT_PROMPT,
    PROJECT_MANAGER_PROMPT,
    REPORT_GENERATOR_PROMPT,
    STRATEGIST_PROMPT,
)


# Utility to safely escape curly braces in prompts when used with ChatPromptTemplate
# to prevent accidental variable detection (e.g., JSON examples). Keeps specified placeholders.
def _escape_braces_for_template(text: str, keep: List[str] | None = None) -> str:
    if not text:
        return text
    keep = keep or []
    placeholders = {k: f"__PLACEHOLDER_{i}__" for i, k in enumerate(keep)}
    # Temporarily replace kept placeholders
    for k, ph in placeholders.items():
        text = text.replace(f"{{{k}}}", ph)
    # Escape all remaining braces
    text = text.replace("{", "{{").replace("}", "}}")
    # Restore kept placeholders
    for k, ph in placeholders.items():
        text = text.replace(ph, f"{{{k}}}")
    return text


# Load environment variables from .env file
load_dotenv(override=True)

# Configure LangSmith
os.environ["LANGSMITH_API_KEY"] = os.getenv("LANGSMITH_API_KEY", "")
os.environ["LANGSMITH_PROJECT"] = os.getenv("LANGSMITH_PROJECT", "KaggleAgent")
os.environ["LANGSMITH_TRACING"] = os.getenv("LANGSMITH_TRACING", "true")

# Standard LLM instance
# z-ai/glm-4.5-air:free, moonshotai/kimi-k2:free, deepseek/deepseek-chat-v3-0324:free, qwen/qwen3-coder
# google/gemini-2.0-flash-001
model = "openai/gpt-4.1-mini"
llm = ChatOpenAI(
    model=model,
    base_url="https://openrouter.ai/api/v1",
    api_key=os.getenv("OPENROUTER_API_KEY"),
    temperature=0.1,  # Lower temperature for more consistent output
    model_kwargs={
        "response_format": {"type": "json_object"}  # Enable JSON mode when supported
    },
)

# JSON-optimized LLM instance for structured outputs
json_llm_base = ChatOpenAI(
    model=model,
    base_url="https://openrouter.ai/api/v1",
    api_key=os.getenv("OPENROUTER_API_KEY"),
    temperature=0,  # Very low temperature for structured output
    # max_tokens=40000,  # Reduced from 4000 to prevent truncation errors
    model_kwargs={"response_format": {"type": "json_object"}},
)


# Helper function to create structured LLM with schema
def create_structured_llm(schema: dict):
    """Create a structured output LLM with the given schema and JSON validation"""
    return json_llm_base.with_structured_output(schema)


# Validation helper function
def validate_json_response(response, schema_title: str) -> bool:
    """Validate if the response matches expected structure"""
    try:
        # Handle string responses that might be JSON
        if isinstance(response, str):
            logger.warning(
                f"âš ï¸  Received string response for {schema_title}: {response[:100]}..."
            )
            try:
                import json
                import re

                # Clean the response before parsing
                cleaned_response = response.strip()
                # Fix common JSON formatting issues with newlines in field names/values
                cleaned_response = re.sub(r'\n\s+(".*?":)', r" \1", cleaned_response)
                # Remove standalone newlines in JSON values
                cleaned_response = re.sub(
                    r'"\s*\n\s*([^"]*)\s*\n\s*"', r'"\1"', cleaned_response
                )

                parsed = json.loads(cleaned_response)
                if isinstance(parsed, dict):
                    logger.debug(
                        f"âœ… Successfully parsed string JSON response for {schema_title}"
                    )
                    return True
            except json.JSONDecodeError as e:
                logger.warning(
                    f"âš ï¸  String response is not valid JSON for {schema_title}: {e}"
                )
                logger.debug(f"Original response: {response[:200]}...")
                return False

        if hasattr(response, "__dict__") or isinstance(response, dict):
            logger.debug(f"âœ… Valid JSON response for {schema_title}")
            return True
        else:
            logger.warning(
                f"âš ï¸  Invalid JSON response structure for {schema_title}, type: {type(response)}"
            )
            return False
    except Exception as e:
        logger.error(f"âŒ JSON validation error for {schema_title}: {e}")
        return False


# JSON mode effectiveness monitoring
def log_json_mode_stats(stats: dict = None):
    """Log statistics about JSON mode effectiveness"""
    if stats is None:
        stats = {
            "total_requests": 0,
            "json_success": 0,
            "fallback_used": 0,
            "validation_failures": 0,
        }

    success_rate = (
        (stats["json_success"] / stats["total_requests"] * 100)
        if stats["total_requests"] > 0
        else 0
    )

    logger.debug("=" * 50)
    logger.debug("ğŸ“Š JSONæ¨¡å¼æ•ˆèƒ½çµ±è¨ˆ")
    logger.debug(f"ç¸½è«‹æ±‚æ•¸: {stats['total_requests']}")
    logger.debug(f"JSONæˆåŠŸ: {stats['json_success']} ({success_rate:.1f}%)")
    logger.debug(f"å¾Œå‚™æ¨¡å¼: {stats['fallback_used']}")
    logger.debug(f"é©—è­‰å¤±æ•—: {stats['validation_failures']}")
    logger.debug("=" * 50)

    if success_rate < 80:
        logger.warning("âš ï¸  JSONæ¨¡å¼æˆåŠŸç‡ä½æ–¼80%ï¼Œå»ºè­°æª¢æŸ¥LLMé…ç½®")
    elif success_rate > 95:
        logger.debug("ğŸ‰ JSONæ¨¡å¼é‹ä½œå„ªç•°ï¼")

    return stats


# Response sanitization and validation for routing
def validate_and_sanitize_strategist_response(
    response, fallback_agent="Feature_Engineer_Agent"
):
    """
    Validate and sanitize strategist response to ensure valid routing.

    Args:
        response: The strategist response object
        fallback_agent: Default agent to route to if validation fails

    Returns:
        dict: Sanitized response with guaranteed valid next_step
    """
    allowed_next_steps = {
        "Data_Analysis_Agent",
        "Feature_Engineer_Agent",
        "Model_Architect_Agent",
        "Report_Generator_Agent",
        "END",
    }

    # Handle different response types
    sanitized_response = {}

    try:
        # Extract response data
        if hasattr(response, "model_dump") and callable(
            getattr(response, "model_dump")
        ):
            sanitized_response = response.model_dump()
        elif hasattr(response, "dict") and callable(getattr(response, "dict")):
            sanitized_response = response.dict()
        elif isinstance(response, dict):
            sanitized_response = response.copy()
        else:
            logger.warning(f"âš ï¸  Unexpected response type: {type(response)}")
            sanitized_response = {
                "next_step": fallback_agent,
                "feedback": str(response),
            }

        # Validate and sanitize next_step
        next_step = sanitized_response.get("next_step", "").strip()

        # Handle common malformed values
        if not next_step or next_step in ["", " ", ":", ": ", "None", "null"]:
            logger.warning(
                f"âš ï¸  Empty or malformed next_step '{next_step}', using fallback: {fallback_agent}"
            )
            next_step = fallback_agent
        elif next_step not in allowed_next_steps:
            logger.warning(
                f"âš ï¸  Invalid next_step '{next_step}', using fallback: {fallback_agent}"
            )
            next_step = fallback_agent

        # Ensure required fields exist
        sanitized_response["next_step"] = next_step
        if "feedback" not in sanitized_response or not sanitized_response["feedback"]:
            sanitized_response["feedback"] = (
                f"Proceeding to {next_step} based on current workflow state."
            )

        # Sanitize other fields
        for field in ["validation_score", "test_score"]:
            if field in sanitized_response and sanitized_response[field] is not None:
                try:
                    sanitized_response[field] = float(sanitized_response[field])
                except (ValueError, TypeError):
                    sanitized_response[field] = None

        logger.debug(f"âœ… Strategist response validated: next_step='{next_step}'")
        return sanitized_response

    except Exception as e:
        logger.error(f"âŒ Response validation failed: {e}")
        return {
            "next_step": fallback_agent,
            "feedback": f"Validation error occurred, proceeding to {fallback_agent}. Error: {str(e)}",
            "validation_score": None,
            "test_score": None,
            "submission_file_path": "",
            "performance_analysis": "",
            "should_continue": True,
            "error_analysis": f"Response validation error: {str(e)}",
            "confidence_level": 0.5,
        }


# Debug and monitoring utilities
def debug_workflow_state(state: dict, checkpoint_name: str = "unknown"):
    """
    Debug utility to log workflow state at key checkpoints.
    """
    logger.debug(f"=== å·¥ä½œæµç¨‹ç‹€æ…‹æª¢æŸ¥é»: {checkpoint_name} ===")

    # Log key state information
    key_fields = [
        "target_column",
        "validation_score",
        "test_score",
        "submission_file_path",
        "last_code_generating_agent",
        "error_count",
        "next_node_after_triage",
    ]

    for field in key_fields:
        value = state.get(field)
        if value is not None:
            logger.debug(f"  {field}: {value}")

    # Check strategist decision
    strategist_decision = state.get("strategist_decision")
    if strategist_decision:
        next_step = strategist_decision.get("next_step")
        logger.debug(f"  strategist_next_step: {next_step}")

    # Check available files
    available_files = state.get("available_files", {})
    if available_files:
        total_files = sum(len(files) for files in available_files.values())
        logger.debug(f"  total_available_files: {total_files}")
        for folder, files in available_files.items():
            if files:
                logger.debug(f"    {folder}: {len(files)} files")

    logger.debug("=" * 50)


def validate_llm_response_structure(response, expected_schema: str, agent_name: str):
    """
    Enhanced validation for LLM responses with detailed debugging.
    """
    logger.debug(f"ğŸ” Validating {agent_name} response structure for {expected_schema}")

    # Check response type
    logger.debug(f"  Response type: {type(response)}")

    # Try to extract content
    if hasattr(response, "content"):
        logger.debug(f"  Response has content: {len(str(response.content))} chars")
        if isinstance(response.content, str):
            # Check if it looks like JSON
            content = response.content.strip()
            if content.startswith("{") and content.endswith("}"):
                logger.debug("  Content appears to be JSON format")
            else:
                logger.warning("  Content does not appear to be JSON format")

    # Check for structured output attributes
    if hasattr(response, "model_dump"):
        logger.debug("  Response has model_dump method (Pydantic v2)")
    elif hasattr(response, "dict"):
        logger.debug("  Response has dict method (Pydantic v1)")
    elif isinstance(response, dict):
        logger.debug("  Response is already a dict")
    else:
        logger.warning(f"  Unknown response structure: {dir(response)}")

    return True


# Enhanced error tracking
def track_agent_error(agent_name: str, error_type: str, error_details: str):
    """
    Enhanced error tracking with better categorization.
    """
    increment_error_stats(error_type, agent_name)

    # Log structured error information
    logger.error(f"ğŸš¨ {agent_name} Error Detected:")
    logger.error(f"   Type: {error_type}")
    logger.error(f"   Details: {error_details[:200]}...")

    # Add to global error tracking
    current_time = pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")
    error_entry = {
        "timestamp": current_time,
        "agent": agent_name,
        "type": error_type,
        "details": error_details[:500],  # Truncate long errors
    }

    # Store in global error log (if needed for analysis)
    if not hasattr(track_agent_error, "error_log"):
        track_agent_error.error_log = []
    track_agent_error.error_log.append(error_entry)

    # Keep only last 50 errors to prevent memory bloat
    if len(track_agent_error.error_log) > 50:
        track_agent_error.error_log = track_agent_error.error_log[-50:]


# é…ç½®æ—¥å¿—ç³»ç»Ÿ
def setup_logging():
    """è¨­ç½®å¢å¼·çš„æ—¥å¿—è¨˜éŒ„ç³»çµ± - å°ˆæ³¨æ–¼éŒ¯èª¤æª¢æ¸¬"""
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    logger = logging.getLogger("kaggle_agent")
    logger.setLevel(logging.DEBUG)  # è¨­ç½®ç‚ºDEBUGä»¥æ•ç²æ‰€æœ‰ç´šåˆ¥

    # å¦‚æœloggerå·²ç»æœ‰handlersï¼Œå…ˆæ¸…é™¤
    if logger.handlers:
        logger.handlers.clear()

    # å‰µå»ºæ–‡ä»¶è™•ç†å™¨ - ä¿å­˜è©³ç´°æ—¥å¿—ç”¨æ–¼èª¿è©¦
    file_handler = logging.FileHandler(
        "kaggle_agent.log", mode="w", encoding="utf-8"
    )  # ä½¿ç”¨'w'æ¨¡å¼é‡æ–°é–‹å§‹
    file_handler.setLevel(logging.DEBUG)

    # å‰µå»ºæ§åˆ¶å°è™•ç†å™¨ - åªé¡¯ç¤ºè­¦å‘Šå’ŒéŒ¯èª¤
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.WARNING)  # åªé¡¯ç¤ºWARNINGåŠä»¥ä¸Šç´šåˆ¥

    # å‰µå»ºä¸åŒçš„æ ¼å¼åŒ–å™¨
    # æ–‡ä»¶ä½¿ç”¨è©³ç´°æ ¼å¼
    file_formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    # æ§åˆ¶å°ä½¿ç”¨ç°¡æ½”æ ¼å¼ï¼Œé‡é»çªå‡ºéŒ¯èª¤
    console_formatter = logging.Formatter("%(levelname)s: %(message)s")

    file_handler.setFormatter(file_formatter)
    console_handler.setFormatter(console_formatter)

    # æ·»åŠ å¤„ç†å™¨åˆ°logger
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger


# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
logger = setup_logging()

# éŒ¯èª¤çµ±è¨ˆè¿½è¹¤
error_stats = {
    "total_errors": 0,
    "syntax_errors": 0,
    "import_errors": 0,
    "runtime_errors": 0,
    "file_errors": 0,
    "last_agent_errors": {},
}


def log_error_stats():
    """è¨˜éŒ„éŒ¯èª¤çµ±è¨ˆæ‘˜è¦"""
    if error_stats["total_errors"] > 0:
        logger.warning(f"ğŸš¨ éŒ¯èª¤çµ±è¨ˆç¸½çµ:")
        logger.warning(f"   ç¸½éŒ¯èª¤æ•¸: {error_stats['total_errors']}")
        logger.warning(f"   èªæ³•éŒ¯èª¤: {error_stats['syntax_errors']}")
        logger.warning(f"   å°å…¥éŒ¯èª¤: {error_stats['import_errors']}")
        logger.warning(f"   é‹è¡Œæ™‚éŒ¯èª¤: {error_stats['runtime_errors']}")
        logger.warning(f"   æ–‡ä»¶éŒ¯èª¤: {error_stats['file_errors']}")
        if error_stats["last_agent_errors"]:
            logger.warning(f"   å„ä»£ç†éŒ¯èª¤æ¬¡æ•¸: {error_stats['last_agent_errors']}")


def increment_error_stats(error_type: str, agent_name: str):
    """å¢åŠ éŒ¯èª¤çµ±è¨ˆè¨ˆæ•¸"""
    error_stats["total_errors"] += 1

    # æŒ‰éŒ¯èª¤é¡å‹åˆ†é¡
    if "syntax" in error_type:
        error_stats["syntax_errors"] += 1
    elif "import" in error_type:
        error_stats["import_errors"] += 1
    elif "file" in error_type:
        error_stats["file_errors"] += 1
    else:
        error_stats["runtime_errors"] += 1

    # æŒ‰ä»£ç†åˆ†é¡
    if agent_name not in error_stats["last_agent_errors"]:
        error_stats["last_agent_errors"][agent_name] = 0
    error_stats["last_agent_errors"][agent_name] += 1


# --- JSON Schema Definitions for Structured Output ---

# Project Manager Schema
PROJECT_MANAGER_SCHEMA = {
    "title": "ProjectManagerResponse",
    "type": "object",
    "properties": {
        "target_column": {
            "title": "Target Column",
            "type": "string",
            "description": "æ¨æ–·çš„ç›®æ¨™æ¬„ä½åç¨±",
        },
        "plan": {
            "title": "Project Plan",
            "type": "string",
            "description": "è©³ç´°çš„å°ˆæ¡ˆè¨ˆç•«",
        },
        "problem_type": {
            "title": "Problem Type",
            "type": "string",
            "description": "å•é¡Œé¡å‹ï¼ˆåˆ†é¡ã€å›æ­¸ç­‰ï¼‰",
        },
        "evaluation_metric": {
            "title": "Evaluation Metric",
            "type": "string",
            "description": "è©•ä¼°æŒ‡æ¨™ï¼ˆAUCã€Accuracyç­‰ï¼‰",
        },
        "next_task_description": {
            "title": "Next Task Description",
            "type": "string",
            "description": "çµ¦ä¸‹ä¸€å€‹ä»£ç†çš„ä»»å‹™æè¿°",
        },
    },
    "required": ["target_column", "plan", "next_task_description"],
}

# Code-Generating Agent Schema
CODE_AGENT_SCHEMA = {
    "title": "CodeAgentResponse",
    "type": "object",
    "properties": {
        "code_to_execute": {
            "title": "Code to Execute",
            "type": "string",
            "description": "è¦åŸ·è¡Œçš„Pythonç¨‹å¼ç¢¼",
        },
        "description": {
            "title": "Code Description",
            "type": "string",
            "description": "ç¨‹å¼ç¢¼åŠŸèƒ½æè¿°",
        },
        "expected_outputs": {
            "title": "Expected Outputs",
            "type": "array",
            "items": {"type": "string"},
            "description": "é æœŸçš„è¼¸å‡ºæª”æ¡ˆæˆ–çµæœ",
        },
        "dependencies": {
            "title": "Dependencies",
            "type": "array",
            "items": {"type": "string"},
            "description": "æ‰€éœ€çš„Pythonå¥—ä»¶",
        },
        "data_sources": {
            "title": "Data Sources",
            "type": "array",
            "items": {"type": "string"},
            "description": "ä½¿ç”¨çš„è³‡æ–™ä¾†æºæª”æ¡ˆ",
        },
        "output_files": {
            "title": "Output Files",
            "type": "array",
            "items": {"type": "string"},
            "description": "å°‡è¦ç”Ÿæˆçš„æª”æ¡ˆè·¯å¾‘",
        },
    },
    "required": ["code_to_execute", "description"],
}

# Enhanced Strategist Schema (extending the existing one)
ENHANCED_STRATEGIST_SCHEMA = {
    "title": "StrategistDecision",
    "type": "object",
    "properties": {
        "next_step": {
            "title": "Next Step",
            "type": "string",
            "enum": [
                "Data_Analysis_Agent",
                "Feature_Engineer_Agent",
                "Model_Architect_Agent",
                "Report_Generator_Agent",
                "END",
            ],
            "description": "ä¸‹ä¸€å€‹è¦å‘¼å«çš„ä»£ç†æˆ–ENDçµæŸæµç¨‹",
        },
        "feedback": {"title": "Feedback", "type": "string"},
        "validation_score": {
            "title": "Validation Score",
            "type": ["number", "null"],
            "description": "é©—è­‰åˆ†æ•¸ï¼Œå¾åŸ·è¡Œè¼¸å‡ºä¸­è§£æå‡ºä¾†ï¼Œå¦‚æœæ²’æœ‰æ‰¾åˆ°å‰‡ç‚ºnull",
        },
        "test_score": {
            "title": "Test Score",
            "type": ["number", "null"],
            "description": "æ¸¬è©¦åˆ†æ•¸ï¼Œå¾åŸ·è¡Œè¼¸å‡ºä¸­è§£æå‡ºä¾†ï¼Œå¦‚æœæ²’æœ‰æ‰¾åˆ°å‰‡ç‚ºnull",
        },
        "submission_file_path": {
            "title": "Submission File Path",
            "type": "string",
            "description": "æäº¤æª”æ¡ˆè·¯å¾‘ï¼Œå¾åŸ·è¡Œè¼¸å‡ºä¸­è§£æå‡ºä¾†ï¼Œå¦‚æœæ²’æœ‰æ‰¾åˆ°å‰‡ç‚ºç©ºå­—ä¸²",
        },
        "performance_analysis": {
            "title": "Performance Analysis",
            "type": "string",
            "description": "å°æ¨¡å‹æ€§èƒ½çš„åˆ†æï¼ŒåŒ…æ‹¬é©—è­‰åˆ†æ•¸å’Œæ¸¬è©¦åˆ†æ•¸çš„è©•ä¼°ã€æ˜¯å¦éæ“¬åˆã€æ˜¯å¦éœ€è¦å„ªåŒ–ç­‰",
        },
        "should_continue": {
            "title": "Should Continue",
            "type": "boolean",
            "description": "åŸºæ–¼æ€§èƒ½åˆ†æï¼Œåˆ¤æ–·æ˜¯å¦æ‡‰è©²ç¹¼çºŒå„ªåŒ–æ¨¡å‹é‚„æ˜¯çµæŸæµç¨‹",
        },
        "error_analysis": {
            "title": "Error Analysis",
            "type": "string",
            "description": "å°éŒ¯èª¤çš„åˆ†æå’Œå»ºè­°çš„è§£æ±ºæ–¹æ¡ˆ",
        },
        "confidence_level": {
            "title": "Confidence Level",
            "type": "number",
            "minimum": 0,
            "maximum": 1,
            "description": "å°æ±ºç­–çš„ä¿¡å¿ƒç¨‹åº¦ï¼ˆ0-1ï¼‰",
        },
    },
    "required": ["next_step", "feedback"],
}

# Report Generator Schema
REPORT_GENERATOR_SCHEMA = {
    "title": "ReportGeneratorResponse",
    "type": "object",
    "properties": {
        "code_to_execute": {
            "title": "Code to Execute",
            "type": "string",
            "description": "è¦åŸ·è¡Œçš„Pythonç¨‹å¼ç¢¼ä¾†ç”Ÿæˆå ±å‘Š",
        },
        "report_title": {
            "title": "Report Title",
            "type": "string",
            "description": "å ±å‘Šæ¨™é¡Œ",
        },
        "report_summary": {
            "title": "Report Summary",
            "type": "string",
            "description": "å ±å‘Šæ‘˜è¦",
        },
        "key_findings": {
            "title": "Key Findings",
            "type": "array",
            "items": {"type": "string"},
            "description": "ä¸»è¦ç™¼ç¾åˆ—è¡¨",
        },
        "recommendations": {
            "title": "Recommendations",
            "type": "array",
            "items": {"type": "string"},
            "description": "å»ºè­°åˆ—è¡¨",
        },
        "charts_analyzed": {
            "title": "Charts Analyzed",
            "type": "array",
            "items": {"type": "string"},
            "description": "åˆ†æçš„åœ–è¡¨åˆ—è¡¨",
        },
    },
    "required": ["code_to_execute", "report_title", "report_summary"],
}


class KaggleWorkflowState(TypedDict):
    """
    æè¿°å·¥ä½œæµç¨‹ä¸­æ¯ä¸€æ­¥çš„ç‹€æ…‹ã€‚
    """

    problem_statement: str
    data_path: str
    target_column: str  # ç›®æ¨™æ¬„ä½åç¨±

    plan: str
    eda_report: str
    feature_plan: str
    model_plan: str

    code_to_execute: str
    execution_stdout: str
    execution_stderr: str

    validation_score: float
    test_score: float
    submission_file_path: str

    iteration_history: List[str]
    last_code_generating_agent: str
    current_task_description: str
    strategist_decision: Dict

    # æ–°å¢çš„éŒ¯èª¤è™•ç†èˆ‡æµç¨‹æ§åˆ¶æ¬„ä½
    error_count: int  # è¨ˆæ•¸é€£çºŒéŒ¯èª¤çš„æ¬¡æ•¸
    next_node_after_triage: str  # åˆ†æµç¯€é»æ±ºå®šçš„ä¸‹ä¸€æ­¥

    # æ–°å¢çš„è·¯å¾‘è¿½è¹¤æ¬„ä½
    workspace_paths: Dict  # åŒ…å«æ‰€æœ‰å·¥ä½œç›®éŒ„çš„è·¯å¾‘è³‡è¨Š
    available_files: Dict  # ç•¶å‰å¯ç”¨çš„æª”æ¡ˆæ¸…å–®

    # é¡å¤–çš„æ¨¡å‹ç›¸é—œåƒæ•¸
    feature_columns: List[str]  # è™•ç†å¾Œçš„ç‰¹å¾µæ¬„ä½åˆ—è¡¨
    model_performance: Dict  # å­˜å„²æ¨¡å‹æ€§èƒ½æŒ‡æ¨™
    best_hyperparameters: Dict  # æœ€ä½³è¶…åƒæ•¸
    preprocessing_steps: List[str]  # è¨˜éŒ„å‰è™•ç†æ­¥é©Ÿ


# --- 2. å»ºç«‹æ ¸å¿ƒå·¥å…·ï¼šç¨‹å¼ç¢¼åŸ·è¡Œæ²™ç®± (Create the Core Tool: Code Executor) ---
# **V2 æ›´æ–°**: æ–°å¢äº† _clean_code è¼”åŠ©å‡½å¼ä¾†ç§»é™¤ LLM å¯èƒ½ç”¢ç”Ÿçš„ Markdown æ¨™ç±¤ã€‚


def _clean_code(code: str) -> str:
    """
    è¼”åŠ©å‡½å¼ï¼Œç”¨æ–¼æ¸…ç†ç¨‹å¼ç¢¼å­—ä¸²ã€‚
    åœ¨JSONæ¨¡å¼ä¸‹ä¸»è¦ç”¨æ–¼å¾Œå‚™è™•ç†ï¼Œç§»é™¤ Markdown æ¨™ç±¤ã€‚
    """
    # ç§»é™¤ ```python, ```, etc. (å¾Œå‚™æ¸…ç†ï¼ŒJSONæ¨¡å¼ä¸‹æ‡‰è©²ä¸éœ€è¦)
    if "```python" in code:
        try:
            code = re.search(r"```python\n(.*)```", code, re.DOTALL).group(1)
        except AttributeError:
            # å¦‚æœæ­£å‰‡è¡¨é”å¼å¤±æ•—ï¼Œå˜—è©¦ç°¡å–®æ›¿æ›
            code = code.replace("```python", "").replace("```", "")
    elif "```" in code:
        code = code.replace("```", "")
    return code.strip()


def _categorize_execution_errors(stderr: str, agent_name: str = None) -> Dict:
    """
    åˆ†é¡å’Œåˆ†æä»£ç¢¼åŸ·è¡ŒéŒ¯èª¤
    """
    if not stderr:
        return {
            "has_error": False,
            "error_type": "none",
            "error_details": "",
            "formatted_error": "",
        }

    stderr_lower = stderr.lower()

    # éŒ¯èª¤é¡å‹æª¢æ¸¬
    error_patterns = {
        "syntax_error": ["syntaxerror", "invalid syntax", "unexpected token"],
        "import_error": ["importerror", "modulenotfounderror", "no module named"],
        "file_error": ["filenotfounderror", "no such file", "permission denied"],
        "memory_error": ["memoryerror", "out of memory"],
        "key_error": ["keyerror", "key not found"],
        "attribute_error": ["attributeerror", "has no attribute"],
        "type_error": ["typeerror", "unsupported operand", "argument"],
        "value_error": ["valueerror", "invalid literal", "cannot convert"],
        "index_error": ["indexerror", "list index out of range"],
        "zero_division": ["zerodivisionerror", "division by zero"],
        "runtime_error": ["runtimeerror", "runtime error"],
        "assertion_error": ["assertionerror"],
        "pandas_error": ["keyerror", "dataframeerror", "series"],
        "sklearn_error": ["sklearn", "fit", "transform", "predict"],
    }

    # è­¦å‘Šæ¨¡å¼æª¢æ¸¬
    warning_patterns = [
        "warning",
        "deprecat",
        "future",
        "userwarning",
        "deprecationwarning",
        "futurewarning",
        "pendingdeprecationwarning",
    ]

    # é¦–å…ˆæª¢æŸ¥æ˜¯å¦åªæ˜¯è­¦å‘Š
    is_only_warning = True
    error_keywords = ["error", "exception", "traceback", "failed"]

    for keyword in error_keywords:
        if keyword in stderr_lower:
            # æª¢æŸ¥æ˜¯å¦åœ¨è­¦å‘Šä¸Šä¸‹æ–‡ä¸­
            if not any(warn in stderr_lower for warn in warning_patterns):
                is_only_warning = False
                break

    if is_only_warning:
        return {
            "has_error": False,
            "error_type": "warning",
            "error_details": stderr.strip(),
            "formatted_error": "",
        }

    # ç¢ºå®šéŒ¯èª¤é¡å‹
    detected_error_type = "unknown_error"
    for error_type, patterns in error_patterns.items():
        if any(pattern in stderr_lower for pattern in patterns):
            detected_error_type = error_type
            break

    # æå–é—œéµéŒ¯èª¤ä¿¡æ¯
    error_lines = stderr.split("\n")
    key_error_lines = []
    for line in error_lines:
        line_lower = line.lower().strip()
        if any(
            keyword in line_lower for keyword in ["error:", "exception:", "traceback"]
        ):
            key_error_lines.append(line.strip())
        elif line.strip() and not any(warn in line_lower for warn in warning_patterns):
            # åŒ…å«éç©ºçš„éè­¦å‘Šè¡Œ
            if len(key_error_lines) < 3:  # é™åˆ¶é—œéµè¡Œæ•¸
                key_error_lines.append(line.strip())

    error_summary = "\n".join(key_error_lines) if key_error_lines else stderr.strip()

    return {
        "has_error": True,
        "error_type": detected_error_type,
        "error_details": error_summary,
        "formatted_error": stderr.strip(),
    }


def setup_workspace_structure(base_path: str = "./kaggle_workspace") -> Dict:
    """
    è¨­ç½®å·¥ä½œå€çš„è³‡æ–™å¤¾çµæ§‹ä¸¦è¿”å›è·¯å¾‘è³‡è¨Šã€‚
    """
    logger.debug("æ­£åœ¨è¨­ç½®å·¥ä½œå€çµæ§‹")

    # å‰µå»ºåŸºç¤å·¥ä½œç›®éŒ„
    os.makedirs(base_path, exist_ok=True)

    # å®šç¾©æ‰€æœ‰éœ€è¦çš„å­è³‡æ–™å¤¾ï¼Œä½¿ç”¨ os.path.join ç¢ºä¿è·¯å¾‘æ­£ç¢º
    folders = {
        "workspace": os.path.abspath(base_path),
        "data": os.path.abspath(os.path.join(base_path, "data")),
        "image": os.path.abspath(os.path.join(base_path, "image")),
        "model": os.path.abspath(os.path.join(base_path, "model")),
        "after_preprocessing": os.path.abspath(
            os.path.join(base_path, "after_preprocessing")
        ),
    }

    # å‰µå»ºæ‰€æœ‰è³‡æ–™å¤¾
    for folder_name, folder_path in folders.items():
        os.makedirs(folder_path, exist_ok=True)
        logger.debug(f"å·²å‰µå»º/ç¢ºèªè³‡æ–™å¤¾: {folder_name} -> {folder_path}")

    return folders


def scan_available_files(workspace_paths: Dict) -> Dict:
    """
    Enhanced file scanning with intelligent file detection and categorization.
    """
    available_files = {
        "data": [],
        "image": [],
        "model": [],
        "after_preprocessing": [],
        "workspace": [],
    }

    file_categories = {
        "csv_files": [],
        "image_files": [],
        "model_files": [],
        "processed_files": [],
        "analysis_files": [],
    }

    for folder_name, folder_path in workspace_paths.items():
        if os.path.exists(folder_path):
            try:
                files = [
                    f
                    for f in os.listdir(folder_path)
                    if os.path.isfile(os.path.join(folder_path, f))
                ]
                available_files[folder_name] = files

                # Categorize files by type and purpose
                for file in files:
                    file_lower = file.lower()
                    file_path = os.path.join(folder_path, file)

                    if file_lower.endswith((".csv")):
                        file_categories["csv_files"].append(
                            {
                                "name": file,
                                "path": file_path,
                                "folder": folder_name,
                                "size": (
                                    os.path.getsize(file_path)
                                    if os.path.exists(file_path)
                                    else 0
                                ),
                            }
                        )

                        if "processed" in file_lower:
                            file_categories["processed_files"].append(
                                {"name": file, "path": file_path, "folder": folder_name}
                            )

                    elif file_lower.endswith((".png", ".jpg", ".jpeg", ".gif", ".svg")):
                        file_categories["image_files"].append(
                            {"name": file, "path": file_path, "folder": folder_name}
                        )

                    elif file_lower.endswith(
                        (".pkl", ".joblib", ".model", ".h5", ".pt")
                    ):
                        file_categories["model_files"].append(
                            {"name": file, "path": file_path, "folder": folder_name}
                        )

                    elif file_lower.endswith((".json", ".txt", ".md")):
                        file_categories["analysis_files"].append(
                            {"name": file, "path": file_path, "folder": folder_name}
                        )

            except PermissionError:
                available_files[folder_name] = []

    # Add file categories to the return dict
    available_files["file_categories"] = file_categories
    return available_files


def generate_file_context_string(workspace_paths: Dict, available_files: Dict) -> str:
    """
    Generate a comprehensive file context string for agents.
    """
    context_parts = []

    # Add workspace structure
    context_parts.append("=== CURRENT WORKSPACE STRUCTURE ===")
    for folder_name, folder_path in workspace_paths.items():
        files = available_files.get(folder_name, [])
        context_parts.append(f"{folder_name.upper()} FOLDER: {folder_path}")
        if files:
            for file in files[:10]:  # Limit to first 10 files per folder
                context_parts.append(f"  - {file}")
            if len(files) > 10:
                context_parts.append(f"  ... and {len(files) - 10} more files")
        else:
            context_parts.append("  - (empty)")

    # Add categorized file information
    file_cats = available_files.get("file_categories", {})
    context_parts.append("\n=== AVAILABLE DATA FILES ===")

    csv_files = file_cats.get("csv_files", [])
    if csv_files:
        context_parts.append("CSV Files:")
        for file_info in csv_files:
            size_mb = file_info["size"] / (1024 * 1024) if file_info["size"] > 0 else 0
            context_parts.append(
                f"  - {file_info['name']} ({file_info['folder']} folder, {size_mb:.1f}MB)"
            )

    processed_files = file_cats.get("processed_files", [])
    if processed_files:
        context_parts.append("Processed Data Files:")
        for file_info in processed_files:
            context_parts.append(
                f"  - {file_info['name']} ({file_info['folder']} folder)"
            )

    context_parts.append("\n=== AVAILABLE VISUALIZATION FILES ===")
    image_files = file_cats.get("image_files", [])
    if image_files:
        for file_info in image_files[:15]:  # Limit to first 15 images
            context_parts.append(
                f"  - {file_info['name']} ({file_info['folder']} folder)"
            )
        if len(image_files) > 15:
            context_parts.append(f"  ... and {len(image_files) - 15} more images")

    context_parts.append("\n=== PATH USAGE GUIDELINES ===")
    context_parts.append("- Always use relative paths from the workspace root")
    context_parts.append("- Data files: Use 'data/filename.csv'")
    context_parts.append(
        "- Processed data: Use 'after_preprocessing/filename.csv' or 'processed/filename.csv'"
    )
    context_parts.append("- Save images to: 'image/filename.png'")
    context_parts.append("- Save models to: 'model/filename.pkl'")
    context_parts.append("- For analysis outputs: Check existing files first")

    return "\n".join(context_parts)


def validate_and_suggest_file_paths(
    code: str, workspace_paths: Dict, available_files: Dict
) -> Dict:
    """
    Validate file paths in code and suggest corrections.
    """
    suggestions = []
    corrections = []

    import re

    # Common file path patterns in Python code
    path_patterns = [
        r'pd\.read_csv\([\'"]([^\'"]+)[\'"]',  # pandas read_csv
        r'\.to_csv\([\'"]([^\'"]+)[\'"]',  # pandas to_csv
        r'plt\.savefig\([\'"]([^\'"]+)[\'"]',  # matplotlib savefig
        r'open\([\'"]([^\'"]+)[\'"]',  # file open
        r'[\'"]([^\'"]*.csv)[\'"]',  # any .csv reference
        r'[\'"]([^\'"]*.png)[\'"]',  # any .png reference
        r'[\'"]([^\'"]*.pkl)[\'"]',  # any .pkl reference
    ]

    found_paths = set()
    for pattern in path_patterns:
        matches = re.findall(pattern, code)
        found_paths.update(matches)

    # Check each found path
    file_cats = available_files.get("file_categories", {})
    all_available_files = {}

    # Build a lookup of available files
    for cat_name, files in file_cats.items():
        for file_info in files:
            filename = file_info["name"]
            folder = file_info["folder"]
            all_available_files[filename] = {
                "folder": folder,
                "correct_path": f"{folder}/{filename}",
                "full_path": file_info["path"],
            }

    for path in found_paths:
        path_clean = path.strip()
        if not path_clean:
            continue

        # Check if path exists as specified
        full_path = None
        if os.path.isabs(path_clean):
            full_path = path_clean
        else:
            # Try relative to workspace
            workspace_root = workspace_paths.get("workspace", "")
            full_path = os.path.join(workspace_root, path_clean)

        if full_path and os.path.exists(full_path):
            # Path is valid
            continue

        # Path doesn't exist, try to suggest correction
        filename = os.path.basename(path_clean)

        if filename in all_available_files:
            file_info = all_available_files[filename]
            suggested_path = file_info["correct_path"]

            suggestions.append(
                {
                    "original_path": path_clean,
                    "suggested_path": suggested_path,
                    "reason": f"File exists in {file_info['folder']} folder",
                    "correction": path_clean != suggested_path,
                }
            )

            if path_clean != suggested_path:
                corrections.append({"from": path_clean, "to": suggested_path})
        else:
            # Try fuzzy matching
            similar_files = []
            for available_file in all_available_files.keys():
                if (
                    filename.lower() in available_file.lower()
                    or available_file.lower() in filename.lower()
                ):
                    similar_files.append(available_file)

            if similar_files:
                best_match = similar_files[0]
                file_info = all_available_files[best_match]
                suggestions.append(
                    {
                        "original_path": path_clean,
                        "suggested_path": file_info["correct_path"],
                        "reason": f"Similar file '{best_match}' found in {file_info['folder']} folder",
                        "correction": True,
                    }
                )

    return {
        "suggestions": suggestions,
        "corrections": corrections,
        "validation_passed": len(corrections) == 0,
    }


def apply_path_corrections(code: str, corrections: List[Dict]) -> str:
    """
    Apply path corrections to code.
    """
    corrected_code = code
    for correction in corrections:
        # Use word boundaries to avoid partial matches
        pattern = re.escape(correction["from"])
        replacement = correction["to"]
        corrected_code = re.sub(
            rf"['\"]\\s*{pattern}\\s*['\"]", f'"{replacement}"', corrected_code
        )

    return corrected_code


def detect_file_changes(old_files: Dict, new_files: Dict) -> Dict:
    """
    Detect changes in file system between two file states.
    """
    changes = {
        "new_files": [],
        "deleted_files": [],
        "modified_files": [],
        "summary": "",
    }

    # Check each folder for changes
    all_folders = set(old_files.keys()).union(set(new_files.keys()))

    for folder in all_folders:
        if folder == "file_categories":  # Skip the categories metadata
            continue

        old_folder_files = set(old_files.get(folder, []))
        new_folder_files = set(new_files.get(folder, []))

        # New files
        new_in_folder = new_folder_files - old_folder_files
        for file in new_in_folder:
            changes["new_files"].append(f"{folder}/{file}")

        # Deleted files
        deleted_in_folder = old_folder_files - new_folder_files
        for file in deleted_in_folder:
            changes["deleted_files"].append(f"{folder}/{file}")

    # Generate summary
    total_changes = (
        len(changes["new_files"])
        + len(changes["deleted_files"])
        + len(changes["modified_files"])
    )
    if total_changes > 0:
        changes["summary"] = (
            f"{len(changes['new_files'])} new, {len(changes['deleted_files'])} deleted, {len(changes['modified_files'])} modified"
        )
    else:
        changes["summary"] = "no changes"

    return changes


def create_file_state_backup(state: Dict) -> Dict:
    """
    Create a backup of current file state for rollback purposes.
    """
    backup = {
        "available_files": state.get("available_files", {}),
        "workspace_paths": state.get("workspace_paths", {}),
        "timestamp": pd.Timestamp.now().isoformat(),
    }
    return backup


def restore_file_state_from_backup(backup: Dict) -> Dict:
    """
    Restore file state from backup (for rollback scenarios).
    """
    return {
        "available_files": backup.get("available_files", {}),
        "workspace_paths": backup.get("workspace_paths", {}),
    }


@traceable(name="execute_code_node")
def execute_code(state: KaggleWorkflowState) -> Dict:
    """
    åœ¨ä¸€å€‹å—æ§ç’°å¢ƒä¸­åŸ·è¡Œç¨‹å¼ç¢¼çš„å·¥å…·ç¯€é»ã€‚
    æ”¯æ´å‚³çµ±åŸ·è¡Œå’Œ Docker å®‰å…¨åŸ·è¡Œå…©ç¨®æ¨¡å¼ã€‚
    """
    logger.debug("æ­£åœ¨åŸ·è¡Œç¨‹å¼ç¢¼")

    code = state.get("code_to_execute", "")
    if not code:
        return {"execution_stdout": "", "execution_stderr": "æ²’æœ‰æä¾›ç¨‹å¼ç¢¼ã€‚"}

    # æ™ºèƒ½ç¨‹å¼ç¢¼æ¸…ç†ï¼šJSONæ¨¡å¼ä¸‹é€šå¸¸ä¸éœ€è¦ï¼Œä½†ä¿ç•™ä½œç‚ºå¾Œå‚™
    cleaned_code = _clean_code(code)

    # è¨˜éŒ„æ˜¯å¦éœ€è¦æ¸…ç†ï¼ˆç›£æ§JSONæ¨¡å¼æ•ˆæœï¼‰
    if cleaned_code != code:
        logger.warning(f"âš ï¸  ç¨‹å¼ç¢¼éœ€è¦æ¸…ç†ï¼Œå¯èƒ½JSONæ¨¡å¼æœªæ­£å¸¸å·¥ä½œ")
    else:
        logger.debug("âœ… ç¨‹å¼ç¢¼å·²ç‚ºç´”æ·¨æ ¼å¼ï¼ŒJSONæ¨¡å¼é‹ä½œæ­£å¸¸")

    # Enhanced path validation and correction
    workspace_paths = state.get("workspace_paths", {})
    available_files = state.get("available_files", {})

    if workspace_paths and available_files:
        validation_result = validate_and_suggest_file_paths(
            cleaned_code, workspace_paths, available_files
        )

        if not validation_result["validation_passed"]:
            logger.warning(
                f"âš ï¸  ç™¼ç¾ {len(validation_result['corrections'])} å€‹è·¯å¾‘å•é¡Œï¼Œæ­£åœ¨è‡ªå‹•ä¿®æ­£"
            )
            for suggestion in validation_result["suggestions"]:
                if suggestion["correction"]:
                    logger.debug(
                        f"è·¯å¾‘ä¿®æ­£: {suggestion['original_path']} -> {suggestion['suggested_path']}"
                    )
                    logger.debug(f"åŸå› : {suggestion['reason']}")

            # Apply corrections
            cleaned_code = apply_path_corrections(
                cleaned_code, validation_result["corrections"]
            )
            logger.debug("âœ… è·¯å¾‘ä¿®æ­£å®Œæˆ")
        else:
            logger.debug("âœ… æ‰€æœ‰æª”æ¡ˆè·¯å¾‘å‡æœ‰æ•ˆ")

    return _execute_code_traditional(state, cleaned_code)


def _execute_code_traditional(state: KaggleWorkflowState, cleaned_code: str) -> Dict:
    """ä½¿ç”¨å‚³çµ±æ–¹å¼åŸ·è¡Œç¨‹å¼ç¢¼ï¼Œå¢å¼·éŒ¯èª¤æª¢æ¸¬å’Œåˆ†é¡"""
    import time

    start_time = time.time()

    # ä½¿ç”¨ç‹€æ…‹ä¸­çš„å·¥ä½œç›®éŒ„è·¯å¾‘
    workspace_paths = state.get("workspace_paths", {})
    work_dir_abs = workspace_paths.get(
        "workspace", os.path.abspath("./kaggle_workspace")
    )

    # ç²å–ç•¶å‰åŸ·è¡Œçš„ä»£ç†ä¿¡æ¯ç”¨æ–¼éŒ¯èª¤ä¸Šä¸‹æ–‡
    current_agent = state.get("last_code_generating_agent", "Unknown_Agent")

    # æ›´æ–°å¯ç”¨æª”æ¡ˆæ¸…å–®
    available_files = scan_available_files(workspace_paths)
    code_with_context = cleaned_code

    original_cwd = os.getcwd()
    os.chdir(work_dir_abs)

    stdout_capture = io.StringIO()
    stderr_capture = io.StringIO()

    try:
        # å‰µå»ºåŸ·è¡Œç’°å¢ƒï¼ŒåŒ…å«save_report_fileå‡½æ•¸
        exec_globals = globals().copy()
        exec_globals["save_report_file"] = (
            lambda filename, content, ws_paths=workspace_paths: save_report_file(
                filename, content, ws_paths
            )
        )
        exec_globals["workspace_paths"] = workspace_paths

        with redirect_stdout(stdout_capture), redirect_stderr(stderr_capture):
            exec(code_with_context, exec_globals)

        stdout = stdout_capture.getvalue()
        stderr = stderr_capture.getvalue()
        execution_time = time.time() - start_time

        # å¢å¼·çš„éŒ¯èª¤æª¢æ¸¬å’Œåˆ†é¡
        error_info = _categorize_execution_errors(stderr, current_agent)
        has_real_error = error_info["has_error"]

        # åªåœ¨æœ‰éŒ¯èª¤æ™‚è¨˜éŒ„è©³ç´°ä¿¡æ¯
        if has_real_error:
            increment_error_stats(error_info["error_type"], current_agent)
            logger.error(f"ğŸš¨ ä»£ç¢¼åŸ·è¡ŒéŒ¯èª¤ - {current_agent}")
            logger.error(f"éŒ¯èª¤é¡å‹: {error_info['error_type']}")
            logger.error(f"éŒ¯èª¤è©³æƒ…: {error_info['error_details']}")
            logger.error(f"åŸ·è¡Œæ™‚é–“: {execution_time:.2f}ç§’")
        elif stderr:
            # åªæœ‰è­¦å‘Šæ™‚ä½¿ç”¨warningç´šåˆ¥
            logger.warning(f"âš ï¸  ä»£ç¢¼åŸ·è¡Œè­¦å‘Š - {current_agent}: {stderr.strip()}")

        # æ›´æ–°å¯ç”¨æª”æ¡ˆæ¸…å–®ä¸¦è¨˜éŒ„è®ŠåŒ–
        updated_files = scan_available_files(workspace_paths)

        # Log file changes for monitoring
        old_files = state.get("available_files", {})
        file_changes = detect_file_changes(old_files, updated_files)

        if (
            file_changes["new_files"]
            or file_changes["deleted_files"]
            or file_changes["modified_files"]
        ):
            logger.debug("ğŸ“ æª”æ¡ˆç³»çµ±è®ŠåŒ–æª¢æ¸¬:")
            if file_changes["new_files"]:
                logger.debug(f"  æ–°å¢æª”æ¡ˆ: {file_changes['new_files']}")
            if file_changes["deleted_files"]:
                logger.debug(f"  åˆªé™¤æª”æ¡ˆ: {file_changes['deleted_files']}")
            if file_changes["modified_files"]:
                logger.debug(f"  ä¿®æ”¹æª”æ¡ˆ: {file_changes['modified_files']}")

        return {
            "execution_stdout": stdout,
            "execution_stderr": error_info["formatted_error"] if has_real_error else "",
            "available_files": updated_files,
            "file_changes": file_changes,
        }

    except SyntaxError as e:
        execution_time = time.time() - start_time
        error_message = f"èªæ³•éŒ¯èª¤: {e.msg} (è¡Œ {e.lineno})"
        increment_error_stats("syntax_error", current_agent)
        logger.error(f"ğŸš¨ èªæ³•éŒ¯èª¤ - {current_agent}: {error_message}")
        logger.error(f"åŸ·è¡Œæ™‚é–“: {execution_time:.2f}ç§’")
        return {
            "execution_stdout": stdout_capture.getvalue(),
            "execution_stderr": error_message,
            "available_files": available_files,
            "file_changes": {
                "new_files": [],
                "deleted_files": [],
                "modified_files": [],
                "summary": "error occurred",
            },
        }
    except ImportError as e:
        execution_time = time.time() - start_time
        error_message = f"å°å…¥éŒ¯èª¤: {str(e)}"
        increment_error_stats("import_error", current_agent)
        logger.error(f"ğŸš¨ å°å…¥éŒ¯èª¤ - {current_agent}: {error_message}")
        logger.error(f"åŸ·è¡Œæ™‚é–“: {execution_time:.2f}ç§’")
        return {
            "execution_stdout": stdout_capture.getvalue(),
            "execution_stderr": error_message,
            "available_files": available_files,
            "file_changes": {
                "new_files": [],
                "deleted_files": [],
                "modified_files": [],
                "summary": "error occurred",
            },
        }
    except FileNotFoundError as e:
        execution_time = time.time() - start_time
        error_message = f"æ–‡ä»¶æœªæ‰¾åˆ°: {str(e)}"
        increment_error_stats("file_error", current_agent)
        logger.error(f"ğŸš¨ æ–‡ä»¶éŒ¯èª¤ - {current_agent}: {error_message}")
        logger.error(f"åŸ·è¡Œæ™‚é–“: {execution_time:.2f}ç§’")
        return {
            "execution_stdout": stdout_capture.getvalue(),
            "execution_stderr": error_message,
            "available_files": available_files,
            "file_changes": {
                "new_files": [],
                "deleted_files": [],
                "modified_files": [],
                "summary": "error occurred",
            },
        }
    except Exception as e:
        execution_time = time.time() - start_time
        stderr = stderr_capture.getvalue()
        error_type = type(e).__name__
        error_message = f"é‹è¡Œæ™‚éŒ¯èª¤ ({error_type}): {str(e)}"
        if stderr:
            error_message += f"\nè©³ç´°ä¿¡æ¯:\n{stderr}"

        increment_error_stats("runtime_error", current_agent)
        logger.error(f"ğŸš¨ é‹è¡Œæ™‚éŒ¯èª¤ - {current_agent}: {error_type}")
        logger.error(f"éŒ¯èª¤è©³æƒ…: {str(e)}")
        logger.error(f"åŸ·è¡Œæ™‚é–“: {execution_time:.2f}ç§’")

        return {
            "execution_stdout": stdout_capture.getvalue(),
            "execution_stderr": error_message,
            "available_files": available_files,
            "file_changes": {
                "new_files": [],
                "deleted_files": [],
                "modified_files": [],
                "summary": "error occurred",
            },
        }
    finally:
        os.chdir(original_cwd)


# --- 3. å·¥å…·å‡½å¼ï¼šä¿å­˜å ±å‘Šæª”æ¡ˆ (Tool Function: Save Report File) ---


def save_report_file(filename: str, content: str, workspace_paths: Dict) -> str:
    """
    ä¿å­˜å ±å‘Šæª”æ¡ˆåˆ°å·¥ä½œå€ã€‚
    """
    try:
        # ä½¿ç”¨å·¥ä½œå€æ ¹ç›®éŒ„ä½œç‚ºä¿å­˜ä½ç½®
        workspace_dir = workspace_paths.get("workspace", "./kaggle_workspace")
        file_path = os.path.join(workspace_dir, filename)

        with open(file_path, "w", encoding="utf-8") as f:
            f.write(content)

        logger.info(f"å ±å‘Šå·²ä¿å­˜è‡³: {file_path}")
        return f"Report saved successfully to: {file_path}"
    except Exception as e:
        error_msg = f"Error saving report file: {str(e)}"
        logger.error(error_msg)
        return error_msg


# --- 4. å®šç¾© AI ä»£ç† (Define the AI Agents) ---


def create_agent_node(system_prompt: str, agent_name: str):
    @traceable(name=agent_name)
    def agent_node(state: KaggleWorkflowState) -> Dict:
        logger.debug(f"æ­£åœ¨å‘¼å«ä»£ç†: {agent_name}")
        task_description = state.get("current_task_description", "")

        # Enhanced context with file awareness
        workspace_paths = state.get("workspace_paths", {})
        available_files = state.get("available_files", {})

        # Generate file context for the agent
        file_context = generate_file_context_string(workspace_paths, available_files)

        # Enhance task description with file context
        enhanced_task_description = f"""
{task_description}

{file_context}

IMPORTANT: Use the file paths exactly as shown above. Always check the available files before referencing them in your code.
"""

        # Create structured LLM for code-generating agents
        structured_llm = create_structured_llm(CODE_AGENT_SCHEMA)

        # ç‚ºModel Architectæ³¨å…¥ç›®æ¨™æ¬„ä½è³‡è¨Š
        if agent_name == "Model_Architect_Agent":
            target_column = state.get("target_column", "target")
            # æ›¿æ›æ‰€æœ‰çš„æ¨¡æ¿è®Šæ•¸ï¼Œé¿å…ChatPromptTemplateéŒ¯èª¤
            enhanced_prompt = system_prompt.replace("{target_column}", target_column)
            # è™•ç†ç¤ºä¾‹ä»£ç¢¼ä¸­çš„å››é‡å¤§æ‹¬è™Ÿè®Šæ•¸ï¼ˆå°‡å®ƒå€‘è½‰æ›ç‚ºé›™é‡å¤§æ‹¬è™Ÿä»¥ä¾¿åœ¨f-stringä¸­æ­£ç¢ºé¡¯ç¤ºï¼‰
            enhanced_prompt = enhanced_prompt.replace(
                "{{{{target_col}}}}", "{{target_col}}"
            )
            enhanced_prompt = enhanced_prompt.replace(
                "{{{{possible_targets}}}}", "{{possible_targets}}"
            )

            try:
                # Try structured output first
                response = structured_llm.invoke(
                    [
                        SystemMessage(content=enhanced_prompt),
                        HumanMessage(content=enhanced_task_description),
                    ]
                )

                if validate_json_response(response, agent_name):
                    logger.debug(f"âœ… {agent_name} æˆåŠŸç”¢ç”Ÿçµæ§‹åŒ–å›æ‡‰")
                    return {
                        "code_to_execute": response.get("code_to_execute", ""),
                        "last_code_generating_agent": agent_name,
                    }
                else:
                    raise ValueError("Invalid JSON response")

            except Exception as e:
                logger.error(f"âŒ {agent_name} JSONå›æ‡‰å¤±æ•—: {e}")
                logger.debug(f"ğŸ”„ {agent_name} é™ç´šç‚ºå‚³çµ±æ–‡æœ¬æ¨¡å¼")

                # Fallback to original approach
                sanitized_system_prompt = _escape_braces_for_template(
                    enhanced_prompt, keep=["current_task_description"]
                )
                prompt_template = ChatPromptTemplate.from_messages(
                    [
                        ("system", sanitized_system_prompt),
                        ("human", "{current_task_description}"),
                    ]
                )
                agent = prompt_template | llm
                response = agent.invoke(
                    {"current_task_description": enhanced_task_description}
                )

                return {
                    "code_to_execute": response.content,
                    "last_code_generating_agent": agent_name,
                }
        else:
            try:
                # Try structured output first
                response = structured_llm.invoke(
                    [
                        SystemMessage(content=system_prompt),
                        HumanMessage(content=enhanced_task_description),
                    ]
                )

                if validate_json_response(response, agent_name):
                    logger.debug(f"âœ… {agent_name} æˆåŠŸç”¢ç”Ÿçµæ§‹åŒ–å›æ‡‰")
                    return {
                        "code_to_execute": response.get("code_to_execute", ""),
                        "last_code_generating_agent": agent_name,
                    }
                else:
                    raise ValueError("Invalid JSON response")

            except Exception as e:
                logger.error(f"âŒ {agent_name} JSONå›æ‡‰å¤±æ•—: {e}")
                logger.debug(f"ğŸ”„ {agent_name} é™ç´šç‚ºå‚³çµ±æ–‡æœ¬æ¨¡å¼")

                # Fallback to original approach
                sanitized_system_prompt = _escape_braces_for_template(
                    system_prompt, keep=["current_task_description"]
                )
                prompt_template = ChatPromptTemplate.from_messages(
                    [
                        ("system", sanitized_system_prompt),
                        ("human", "{current_task_description}"),
                    ]
                )
                agent = prompt_template | llm
                response = agent.invoke(
                    {"current_task_description": enhanced_task_description}
                )

                return {
                    "code_to_execute": response.content,
                    "last_code_generating_agent": agent_name,
                }

    return agent_node


# ä»£ç†çš„ Prompt ç¶­æŒä¸è®Š...


def project_manager_node(state: KaggleWorkflowState) -> Dict:
    """Project manager agent node with robust structured response handling.

    Fixes:
    - Safely handle Pydantic BaseModel (uses model_dump / dict)
    - Handle raw string (attempt JSON extraction / fallback)
    - Avoid calling .get on BaseModel directly
    - Trim and sanitize target_column
    - Provide clearer debug logs for troubleshooting
    """
    logger.debug("æ­£åœ¨å‘¼å«ä»£ç†: å°ˆæ¡ˆç¶“ç† (å¢å¼·ç‰ˆ)")
    # NOTE: Avoid using .format on the prompt because it contains JSON braces which
    # would trigger KeyError (e.g., '{"target_column"'). If data_path injection is needed
    # explicitly add a placeholder like '{data_path}' and escape other braces. For now we
    # just use the static prompt.
    prompt = PROJECT_MANAGER_PROMPT  # previously used .format causing KeyError

    structured_llm = create_structured_llm(PROJECT_MANAGER_SCHEMA)

    def _response_to_dict(resp):
        """Normalize different response object types into a plain dict."""
        try:
            # Already dict
            if isinstance(resp, dict):
                return resp
            # Pydantic v2
            if hasattr(resp, "model_dump") and callable(getattr(resp, "model_dump")):
                return resp.model_dump()
            # Pydantic v1
            if hasattr(resp, "dict") and callable(getattr(resp, "dict")):
                return resp.dict()
            # LangChain message with .content
            if hasattr(resp, "content") and isinstance(resp.content, (str, dict)):
                inner = resp.content
                if isinstance(inner, dict):
                    return inner
                # Try to parse JSON from string
                parsed = _extract_json(inner)
                if parsed:
                    return parsed
                return {"raw_text": inner}
            # Raw string
            if isinstance(resp, str):
                parsed = _extract_json(resp)
                if parsed:
                    return parsed
                return {"raw_text": resp}
        except Exception as e:  # noqa: BLE001
            logger.error(f"âŒ è½‰æ›å›æ‡‰ç‚ºå­—å…¸å¤±æ•—: {e}")
        return {}

    def _extract_json(text: str):
        import json
        import re

        if not text:
            return None
        # Try direct load
        try:
            return json.loads(text)
        except Exception:
            pass
        # Extract largest JSON object with braces
        try:
            match = re.search(r"\{[\s\S]*\}", text)
            if match:
                candidate = match.group(0)
                # Clean common issues (newline before key, trailing commas)
                candidate = re.sub(r'\n\s+(".*?":)', r" \1", candidate)
                candidate = re.sub(r",\s*([}\]])", r"\1", candidate)
                return json.loads(candidate)
        except Exception as e:  # noqa: BLE001
            logger.debug(f"JSONæŠ½å–å¤±æ•—: {e}")
        return None

    try:
        response = structured_llm.invoke(
            [
                SystemMessage(content=prompt),
                HumanMessage(content=state.get("problem_statement", "")),
            ]
        )

        logger.debug(f"Project Manager raw response type: {type(response)}")
        logger.debug(
            f"Project Manager dir: {dir(response) if hasattr(response, '__dir__') else 'n/a'}"
        )
        if hasattr(response, "__dict__"):
            logger.debug(f"Project Manager __dict__: {vars(response)}")

        resp_dict = _response_to_dict(response)
        logger.debug(f"Normalized response dict: {resp_dict}")

        if not resp_dict:
            raise ValueError("Empty or unparseable response from structured_llm")

        # If schema fields nested under data key (rare), flatten
        if all(
            k in resp_dict.get("data", {})
            for k in ["target_column", "plan", "next_task_description"]
        ):
            resp_dict = resp_dict["data"]

        # Validate required keys
        missing = [
            k
            for k in ["target_column", "plan", "next_task_description"]
            if k not in resp_dict
        ]
        if missing:
            raise KeyError(
                f"Missing required keys in response: {missing}; raw keys: {list(resp_dict.keys())}"
            )

        target_column = resp_dict.get("target_column") or "target"
        if isinstance(target_column, str):
            target_column = target_column.strip().replace("\n", "").replace("\r", "")
        else:
            target_column = str(target_column)

        plan = resp_dict.get("plan", "")
        next_task = resp_dict.get("next_task_description", plan)

        logger.debug(f"âœ… å°ˆæ¡ˆç¶“ç†æˆåŠŸè­˜åˆ¥ç›®æ¨™æ¬„ä½: {target_column}")
        logger.debug(f"å•é¡Œé¡å‹: {resp_dict.get('problem_type')}")
        logger.debug(f"è©•ä¼°æŒ‡æ¨™: {resp_dict.get('evaluation_metric')}")

        return {
            "current_task_description": next_task,
            "plan": plan,
            "target_column": target_column,
        }

    except Exception as e:  # noqa: BLE001
        logger.error(f"âŒ å°ˆæ¡ˆç¶“ç†çµæ§‹åŒ–è™•ç†å¤±æ•—: {e}")
        logger.debug("ğŸ”„ é™ç´šç‚ºå‚³çµ±æ–‡æœ¬è§£ææ¨¡å¼")
        # Fallback: simple LLM invocation and heuristic parsing
        fallback_resp = llm.invoke(
            [
                SystemMessage(content=prompt),
                HumanMessage(content=state.get("problem_statement", "")),
            ]
        )
        raw_text = getattr(fallback_resp, "content", str(fallback_resp))

        # Try to extract target column heuristically
        heur_target = "target"
        patterns = [
            r"target_column\s*[:=]\s*['\"]([^'\"]+)['\"]",
            r"TARGET_COLUMN:\s*(\w+)",
        ]
        import re

        for pat in patterns:
            m = re.search(pat, raw_text, re.IGNORECASE)
            if m:
                heur_target = m.group(1)
                break
        heur_target = heur_target.strip().replace("\n", "")
        logger.debug(f"é™ç´šæ¨¡å¼æ¨æ–·ç›®æ¨™æ¬„ä½: {heur_target}")
        return {
            "current_task_description": raw_text,
            "plan": raw_text,
            "target_column": heur_target or "target",
        }


data_analysis_agent = create_agent_node(DATA_ANALYST_PROMPT, "Data_Analysis_Agent")

feature_engineer_agent = create_agent_node(
    FEATURE_ENGINEER_PROMPT, "Feature_Engineer_Agent"
)

model_architect_agent = create_agent_node(
    MODEL_ARCHITECT_PROMPT, "Model_Architect_Agent"
)

# å ±å‘Šæ’°å¯«ä»£ç†


def report_generator_node(state: KaggleWorkflowState) -> Dict:
    """å°ˆé–€è™•ç†å ±å‘Šç”Ÿæˆçš„ç¯€é»ï¼Œå…·å‚™å¢å¼·çš„SHAPåˆ†æå’Œè‡ªå‹•åŒ–æ´å¯Ÿæå–"""
    logger.debug("æ­£åœ¨å‘¼å«ä»£ç†: å ±å‘Šæ’°å¯«ä»£ç†")

    # Create structured LLm for Report Generator
    structured_llm = create_structured_llm(REPORT_GENERATOR_SCHEMA)

    # Enhanced context with file awareness
    workspace_paths = state.get("workspace_paths", {})
    available_files = state.get("available_files", {})

    # æº–å‚™ä¸Šä¸‹æ–‡è³‡è¨Š
    context_info = []

    # è§£æåŸ·è¡Œè¼¸å‡ºä¸­çš„é—œéµä¿¡æ¯
    execution_stdout = state.get("execution_stdout", "")

    # æå–SHAPåˆ†æçµæœ
    shap_insights = ""
    if "=== SHAP Analysis Insights ===" in execution_stdout:
        shap_start = execution_stdout.find("=== SHAP Analysis Insights ===")
        shap_end = execution_stdout.find(
            "=== Automated Insights & Recommendations ===", shap_start
        )
        if shap_end == -1:
            shap_insights = execution_stdout[shap_start:]
        else:
            shap_insights = execution_stdout[shap_start:shap_end]

    # æå–è‡ªå‹•åŒ–æ´å¯Ÿå’Œå»ºè­°
    automated_insights = ""
    if "=== Automated Insights & Recommendations ===" in execution_stdout:
        insights_start = execution_stdout.find(
            "=== Automated Insights & Recommendations ==="
        )
        automated_insights = execution_stdout[insights_start:]

    # æ·»åŠ åŸ·è¡Œè¼¸å‡ºï¼ˆåŒ…å«è§£æçš„é—œéµéƒ¨åˆ†ï¼‰
    if execution_stdout:
        context_info.append(f"Model Training Output:\n{execution_stdout}\n")

    if shap_insights:
        context_info.append(f"Extracted SHAP Insights:\n{shap_insights}\n")

    if automated_insights:
        context_info.append(f"Extracted Automated Insights:\n{automated_insights}\n")

    # æ·»åŠ åˆ†æ•¸è³‡è¨Š
    if state.get("validation_score"):
        context_info.append(f"Validation Score: {state['validation_score']}\n")
    if state.get("test_score"):
        context_info.append(f"Test Score: {state['test_score']}\n")

    # æ™ºèƒ½åˆ†æå¯ç”¨æª”æ¡ˆ
    available_files = state.get("available_files", {})
    image_files = available_files.get("image", [])

    # åˆ†é¡åœ–è¡¨é¡å‹
    chart_analysis = {
        "shap_charts": [],
        "traditional_charts": [],
        "performance_charts": [],
    }

    for img in image_files:
        if "shap" in img.lower():
            chart_analysis["shap_charts"].append(img)
        elif "confusion_matrix" in img.lower() or "roc" in img.lower():
            chart_analysis["performance_charts"].append(img)
        else:
            chart_analysis["traditional_charts"].append(img)

    context_info.append(f"Available Chart Analysis:\n")
    context_info.append(f"- SHAP Charts: {chart_analysis['shap_charts']}\n")
    context_info.append(
        f"- Performance Charts: {chart_analysis['performance_charts']}\n"
    )
    context_info.append(
        f"- Traditional Charts: {chart_analysis['traditional_charts']}\n"
    )

    # æ·»åŠ è¿­ä»£æ­·å²
    if state.get("iteration_history"):
        context_info.append("Workflow History:\n")
        for item in state["iteration_history"][-5:]:  # åªå–æœ€è¿‘5å€‹
            context_info.append(f"- {item}\n")

    context_str = "\n".join(context_info)

    # Generate file context for the report generator
    file_context = generate_file_context_string(workspace_paths, available_files)

    # æ§‹å»ºå®Œæ•´çš„ä»»å‹™æè¿°
    task_description = f"""
Based on the analysis workflow results, generate a comprehensive data science report with enhanced SHAP interpretability and automated insights.

Current State Information:
{context_str}

Current File System State:
{file_context}

Special Instructions for Enhanced Reporting:
1. Prioritize SHAP visualizations in the report structure
2. Extract and interpret automated insights from the execution output
3. Include quantitative analysis of feature importance comparisons
4. Generate business-actionable recommendations based on SHAP patterns
5. Address model deployment readiness and risk assessment

Please respond in JSON format with:
- code_to_execute: Python code to generate the report
- report_title: Title of the analysis report
- report_summary: Executive summary of findings
- key_findings: List of main discoveries
- recommendations: List of actionable recommendations
- charts_analyzed: List of charts that will be analyzed in the report
"""

    try:
        # Try structured output first
        response = structured_llm.invoke(
            [
                SystemMessage(content=REPORT_GENERATOR_PROMPT),
                HumanMessage(content=task_description),
            ]
        )

        if validate_json_response(response, "ReportGenerator"):
            logger.debug("âœ… å ±å‘Šç”Ÿæˆä»£ç†æˆåŠŸç”¢ç”Ÿçµæ§‹åŒ–å›æ‡‰")
            return {
                "code_to_execute": response.get("code_to_execute", ""),
                "last_code_generating_agent": "Report_Generator_Agent",
            }
        else:
            raise ValueError("Invalid JSON response")

    except Exception as e:
        logger.error(f"âŒ å ±å‘Šç”Ÿæˆä»£ç†JSONå›æ‡‰å¤±æ•—: {e}")
        logger.debug("ğŸ”„ å ±å‘Šç”Ÿæˆä»£ç†é™ç´šç‚ºå‚³çµ±æ–‡æœ¬æ¨¡å¼")

        # Fallback to original approach
        sanitized_report_prompt = _escape_braces_for_template(
            REPORT_GENERATOR_PROMPT, keep=["task_description"]
        )
        prompt_template = ChatPromptTemplate.from_messages(
            [("system", sanitized_report_prompt), ("human", "{task_description}")]
        )
        agent = prompt_template | llm
        response = agent.invoke({"task_description": task_description})

        return {
            "code_to_execute": response.content,
            "last_code_generating_agent": "Report_Generator_Agent",
        }


# **V2 æ›´æ–°**: å¢å¼·äº†ç­–ç•¥å¸«çš„ Promptï¼Œä½¿å…¶èƒ½å¤ è™•ç†ä¾†è‡ª Triage ç¯€é»çš„å‡ç´šå•é¡Œã€‚


def chief_strategist_node(state: KaggleWorkflowState) -> Dict:
    logger.debug("æ­£åœ¨å‘¼å«ä»£ç†: é¦–å¸­ç­–ç•¥å¸«")
    context_list = []
    for key, value in state.items():
        if key not in ["strategist_decision"] and value:
            if isinstance(value, list) and value:
                context_list.append(f"- {key}:\n  - " + "\n  - ".join(map(str, value)))
            elif isinstance(value, str) and value.strip():
                context_list.append(f"- {key}:\n{value}\n")
            elif isinstance(value, (float, int)):
                context_list.append(f"- {key}: {value}")
    context_str = "\n".join(context_list)
    prompt = STRATEGIST_PROMPT.format(context_str=context_str)
    logger.debug(f"ç­–ç•¥å¸«çš„strategist_prompt: {context_str}")
    # Use the enhanced strategist schema with better validation
    json_llm = create_structured_llm(ENHANCED_STRATEGIST_SCHEMA)

    try:
        response = json_llm.invoke(
            [
                SystemMessage(content=prompt),
                HumanMessage(
                    content="è«‹æ ¹æ“šä»¥ä¸Šä¸Šä¸‹æ–‡ï¼Œåšå‡ºä½ çš„ä¸‹ä¸€æ­¥æ±ºç­–ã€‚ç‰¹åˆ¥æ³¨æ„è§£æåŸ·è¡Œè¼¸å‡ºä¸­çš„ 'Validation Score:', 'Test Score:' å’Œ 'Submission file saved to:' è³‡è¨Šã€‚"
                ),
            ]
        )

        if not validate_json_response(response, "StrategistDecision"):
            raise ValueError("Invalid JSON response structure")

        logger.debug(f"âœ… ç­–ç•¥å¸«æˆåŠŸç”¢ç”Ÿçµæ§‹åŒ–æ±ºç­–: {response}")

    except Exception as e:
        logger.error(f"âŒ ç­–ç•¥å¸«JSONå›æ‡‰å¤±æ•—: {e}")
        logger.debug("ğŸ”„ ç­–ç•¥å¸«é™ç´šç‚ºåŸºç¤çµæ§‹åŒ–è¼¸å‡ºæ¨¡å¼")

        # Fallback to basic structured output
        fallback_schema = {
            "title": "BasicStrategistDecision",
            "type": "object",
            "properties": {
                "next_step": {"title": "Next Step", "type": "string"},
                "feedback": {"title": "Feedback", "type": "string"},
                "validation_score": {
                    "title": "Validation Score",
                    "type": ["number", "null"],
                    "description": "é©—è­‰åˆ†æ•¸",
                },
                "test_score": {
                    "title": "Test Score",
                    "type": ["number", "null"],
                    "description": "æ¸¬è©¦åˆ†æ•¸",
                },
                "submission_file_path": {
                    "title": "Submission File Path",
                    "type": "string",
                    "description": "æäº¤æª”æ¡ˆè·¯å¾‘",
                },
                "performance_analysis": {
                    "title": "Performance Analysis",
                    "type": "string",
                    "description": "æ€§èƒ½åˆ†æ",
                },
                "should_continue": {
                    "title": "Should Continue",
                    "type": "boolean",
                    "description": "æ˜¯å¦ç¹¼çºŒ",
                },
            },
            "required": ["next_step", "feedback"],
        }

        fallback_llm = json_llm_base.with_structured_output(fallback_schema)
        response = fallback_llm.invoke(
            [
                SystemMessage(content=prompt),
                HumanMessage(
                    content="è«‹æ ¹æ“šä»¥ä¸Šä¸Šä¸‹æ–‡ï¼Œåšå‡ºä½ çš„ä¸‹ä¸€æ­¥æ±ºç­–ã€‚ç‰¹åˆ¥æ³¨æ„è§£æåŸ·è¡Œè¼¸å‡ºä¸­çš„ 'Validation Score:', 'Test Score:' å’Œ 'Submission file saved to:' è³‡è¨Šã€‚"
                ),
            ]
        )
        logger.debug(f"ğŸ”„ ç­–ç•¥å¸«é™ç´šæ±ºç­–: {response}")

    # Validate and sanitize response to prevent routing errors
    response = validate_and_sanitize_strategist_response(response)
    logger.debug(f"âœ… ç­–ç•¥å¸«å›æ‡‰å·²é©—è­‰å’Œæ¸…ç†: {response}")

    new_history = state.get("iteration_history", [])

    # ä½¿ç”¨é©—è­‰å¾Œçš„çµæœ
    validation_score = response.get("validation_score")
    test_score = response.get("test_score")
    submission_file_path = response.get("submission_file_path")
    performance_analysis = response.get("performance_analysis", "")
    # should_continue ç›®å‰æœªä½¿ç”¨ï¼Œå¯åœ¨æœªä¾†æ“´å±•ä¸­å¯¦ç¾è‡ªå‹•åŒ–æµç¨‹æ§åˆ¶

    # æ›´æ–°æ­·å²è¨˜éŒ„å’Œè¿”å›å€¼
    result = {
        "strategist_decision": response,
        "current_task_description": response["feedback"],
        "iteration_history": new_history,
    }

    if validation_score is not None:
        result["validation_score"] = validation_score
        new_history.append(f"æ¨¡å‹è¨“ç·´å®Œæˆï¼Œé©—è­‰åˆ†æ•¸: {validation_score}")

    if test_score is not None:
        result["test_score"] = test_score
        new_history.append(f"æ¸¬è©¦åˆ†æ•¸: {test_score}")

    if submission_file_path:
        result["submission_file_path"] = submission_file_path
        new_history.append(f"æäº¤æª”æ¡ˆå·²ä¿å­˜: {submission_file_path}")

    if performance_analysis:
        new_history.append(f"æ€§èƒ½åˆ†æ: {performance_analysis}")

    if not any([validation_score, test_score, submission_file_path]):
        new_history.append(
            f"ç­–ç•¥å¸«æ±ºç­–: {response['next_step']} - {response['feedback'][:50]}..."
        )

    result["iteration_history"] = new_history
    return result


# --- 4. å®šç¾©æµç¨‹åœ–çš„é‚Šå’Œæ¢ä»¶é‚è¼¯ (Define Graph Edges & Conditional Logic) ---
# **V2 æ›´æ–°**: æ–°å¢äº† Triage ç¯€é»å’Œå°æ‡‰çš„ Routerï¼Œå–ä»£äº†èˆŠçš„æ¢ä»¶å¼é‚Šã€‚


def triage_node(state: KaggleWorkflowState) -> Dict:
    """
    åˆ†æåŸ·è¡Œçµæœï¼Œæ±ºå®šä¸‹ä¸€æ­¥æ˜¯ä¿®æ­£ã€è©•ä¼°é‚„æ˜¯å‡ç´šå•é¡Œã€‚
    """
    if state.get("execution_stderr"):
        logger.debug("åµæ¸¬åˆ°ç¨‹å¼ç¢¼éŒ¯èª¤ï¼Œé€²è¡Œåˆ†æµ")
        error_count = state.get("error_count", 0) + 1

        # å¦‚æœé€£çºŒéŒ¯èª¤é”åˆ° 2 æ¬¡ï¼Œå°‡å•é¡Œå‡ç´šçµ¦ç­–ç•¥å¸«
        if error_count >= 2:
            logger.warning(f"éŒ¯èª¤æ¬¡æ•¸é”åˆ° {error_count}ï¼Œå°‡å•é¡Œå‡ç´šçµ¦ç­–ç•¥å¸«")
            feedback = (
                f"ä»£ç† '{state['last_code_generating_agent']}' é€£çºŒå¤šæ¬¡ç„¡æ³•ä¿®æ­£å…¶ç¨‹å¼ç¢¼éŒ¯èª¤ã€‚\n"
                f"é€™æ˜¯æœ€å¾Œä¸€æ¬¡çš„éŒ¯èª¤è¨Šæ¯ï¼š\n{state['execution_stderr']}\n"
                f"è«‹åˆ†ææ ¹æœ¬åŸå› ï¼Œä¸¦åˆ¶å®šä¸€å€‹å…¨æ–°çš„è¨ˆç•«ä¾†æ‰“ç ´åƒµå±€ã€‚"
            )
            return {
                "error_count": 0,  # é‡ç½®è¨ˆæ•¸å™¨
                "current_task_description": feedback,
                "execution_stderr": "",  # æ¸…é™¤éŒ¯èª¤ï¼Œå› ç‚ºç¾åœ¨æ˜¯ç­–ç•¥å•é¡Œ
                "next_node_after_triage": "Chief_Strategist_Agent",
            }
        # å¦‚æœéŒ¯èª¤æ¬¡æ•¸å°šåœ¨å®¹è¨±ç¯„åœï¼Œè¿”å›åŸä»£ç†ä¿®æ­£
        else:
            logger.debug(f"ç¬¬ {error_count} æ¬¡éŒ¯èª¤ï¼Œè¿”å›ä¿®æ­£")
            feedback = (
                f"ä½ çš„ä¸Šä¸€æ®µç¨‹å¼ç¢¼åŸ·è¡Œå¤±æ•—ï¼Œè«‹ä¿®æ­£å®ƒã€‚\n"
                f"é€™æ˜¯ç¬¬ {error_count} æ¬¡å˜—è©¦ã€‚\n"
                f"éŒ¯èª¤è¨Šæ¯å¦‚ä¸‹ï¼š\n{state['execution_stderr']}"
            )
            return {
                "error_count": error_count,
                "current_task_description": feedback,
                "next_node_after_triage": state["last_code_generating_agent"],
            }
    else:
        logger.debug("ç¨‹å¼ç¢¼åŸ·è¡ŒæˆåŠŸï¼Œäº¤ç”±ç­–ç•¥å¸«è©•ä¼°")
        return {
            "error_count": 0,  # æˆåŠŸå¾Œé‡ç½®éŒ¯èª¤è¨ˆæ•¸å™¨
            "next_node_after_triage": "Chief_Strategist_Agent",
        }


def router_after_triage(state: KaggleWorkflowState):
    """æ ¹æ“šåˆ†æµç¯€é»çš„æ±ºå®šï¼Œå°å‘åˆ°ä¸‹ä¸€å€‹ç¯€é»ã€‚"""
    destination = state.get("next_node_after_triage")
    logger.debug(f"åˆ†æµçµæœ: å‰å¾€ {destination}")
    return destination


def router_after_strategy(state: KaggleWorkflowState):
    """æ ¹æ“šé¦–å¸­ç­–ç•¥å¸«çš„æ±ºç­–ï¼Œæ±ºå®šä¸‹ä¸€å€‹ç¯€é»ã€‚
    å¢å¼·ç‰ˆ: å¼·åŒ–éŒ¯èª¤è™•ç†å’Œé©—è­‰ï¼Œå®Œå…¨é˜²æ­¢ KeyError å’Œè·¯ç”±å¤±æ•—ã€‚
    æ¨æ–·é‚è¼¯:
      1. è‹¥å·²æœ‰è™•ç†å¾Œè³‡æ–™ (after_preprocessing/*.csv) -> é€²å…¥å»ºæ¨¡éšæ®µ Model_Architect_Agent
      2. è‹¥å·²æœ‰é©—è­‰èˆ‡æ¸¬è©¦åˆ†æ•¸ (ä¸” >0) -> é€²å…¥å ±å‘Šç”Ÿæˆ Report_Generator_Agent
      3. å¦å‰‡ (åƒ…å®Œæˆ EDA) -> é€²å…¥ç‰¹å¾µå·¥ç¨‹ Feature_Engineer_Agent
    """
    allowed = {
        "Data_Analysis_Agent",
        "Feature_Engineer_Agent",
        "Model_Architect_Agent",
        "Report_Generator_Agent",
        "END",
    }

    # é è¨­å®‰å…¨çš„fallback
    safe_fallback = "Feature_Engineer_Agent"

    try:
        # ç²å–ç­–ç•¥å¸«æ±ºç­–ï¼Œç¢ºä¿å®‰å…¨å­˜å–
        strategist_decision = state.get("strategist_decision")
        if not strategist_decision or not isinstance(strategist_decision, dict):
            logger.warning(
                f"âš ï¸  Missing or invalid strategist_decision, using fallback: {safe_fallback}"
            )
            return safe_fallback

        next_step = strategist_decision.get("next_step")

        # æ¸…ç†å’Œé©—è­‰ next_step å€¼
        if isinstance(next_step, str):
            next_step = next_step.strip()

        # è™•ç†å¸¸è¦‹çš„ç„¡æ•ˆå€¼
        invalid_values = ["", " ", ":", ": ", "None", "null", None]
        if next_step in invalid_values:
            logger.warning(
                f"âš ï¸  Invalid next_step value '{next_step}', inferring from workflow state"
            )
            next_step = None

        # é©—è­‰æ˜¯å¦ç‚ºå…è¨±çš„å€¼
        if next_step and next_step not in allowed:
            logger.warning(
                f"âš ï¸  Unrecognized next_step '{next_step}', inferring from workflow state"
            )
            next_step = None

        # å¦‚æœéœ€è¦æ¨æ–·ä¸‹ä¸€æ­¥
        if not next_step:
            try:
                available_files = state.get("available_files", {}) or {}
                after_pre_files = available_files.get("after_preprocessing", []) or []
                has_processed = any(
                    f.endswith("_processed.csv") for f in after_pre_files
                )
                val_score = state.get("validation_score", 0) or 0
                test_score = state.get("test_score", 0) or 0

                # æ™ºèƒ½æ¨æ–·ä¸‹ä¸€æ­¥
                if (val_score and val_score > 0) and (test_score and test_score > 0):
                    next_step = "Report_Generator_Agent"
                    logger.info(f"ğŸ¤– Inferred next step: {next_step} (has scores)")
                elif has_processed:
                    next_step = "Model_Architect_Agent"
                    logger.info(
                        f"ğŸ¤– Inferred next step: {next_step} (has processed data)"
                    )
                else:
                    next_step = "Feature_Engineer_Agent"
                    logger.info(f"ğŸ¤– Inferred next step: {next_step} (default)")

                # æ›´æ–°ç‹€æ…‹ä»¥è¨˜éŒ„æ¨æ–·çµæœ
                strategist_decision["next_step"] = next_step
                state["strategist_decision"] = strategist_decision

            except Exception as e:
                logger.error(f"âŒ Error during next_step inference: {e}")
                next_step = safe_fallback

        # æœ€çµ‚é©—è­‰ - ç¢ºä¿è¿”å›å€¼çµ•å°å®‰å…¨
        if next_step not in allowed:
            logger.error(
                f"âŒ Final validation failed for next_step '{next_step}', using safe fallback: {safe_fallback}"
            )
            next_step = safe_fallback

        logger.debug(f"âœ… Router decision: {next_step}")
        return END if next_step == "END" else next_step

    except Exception as e:
        logger.error(f"âŒ Critical error in router_after_strategy: {e}")
        logger.warning(f"ğŸš¨ Using emergency fallback: {safe_fallback}")
        return safe_fallback


# --- 5. çµ„è£ LangGraph æµç¨‹åœ– (Assemble the Graph) ---
# **V2 æ›´æ–°**: ä¿®æ”¹äº†åœ–çš„çµæ§‹ï¼ŒåŠ å…¥äº† Triage ç¯€é»ã€‚

workflow = StateGraph(KaggleWorkflowState)

workflow.add_node("Project_Manager_Agent", project_manager_node)
workflow.add_node("Data_Analysis_Agent", data_analysis_agent)
workflow.add_node("Feature_Engineer_Agent", feature_engineer_agent)
workflow.add_node("Model_Architect_Agent", model_architect_agent)
workflow.add_node("Report_Generator_Agent", report_generator_node)  # æ–°å¢å ±å‘Šæ’°å¯«ä»£ç†
workflow.add_node("Code_Executor_Node", execute_code)
workflow.add_node("Triage_Node", triage_node)  # æ–°å¢åˆ†æµç¯€é»
workflow.add_node("Chief_Strategist_Agent", chief_strategist_node)

workflow.set_entry_point("Project_Manager_Agent")

workflow.add_edge("Project_Manager_Agent", "Data_Analysis_Agent")
workflow.add_edge("Data_Analysis_Agent", "Code_Executor_Node")
workflow.add_edge("Feature_Engineer_Agent", "Code_Executor_Node")
workflow.add_edge("Model_Architect_Agent", "Code_Executor_Node")
workflow.add_edge(
    "Report_Generator_Agent", "Code_Executor_Node"
)  # å ±å‘Šæ’°å¯«ä»£ç†ä¹Ÿéœ€è¦åŸ·è¡Œç¨‹å¼ç¢¼

# ç¨‹å¼ç¢¼åŸ·è¡Œå¾Œï¼Œç¸½æ˜¯å…ˆåˆ° Triage ç¯€é»é€²è¡Œåˆ†æµ
workflow.add_edge("Code_Executor_Node", "Triage_Node")

# Triage ç¯€é»å¾Œçš„æ¢ä»¶å¼è·¯ç”±
workflow.add_conditional_edges(
    "Triage_Node",
    router_after_triage,
    {
        "Data_Analysis_Agent": "Data_Analysis_Agent",
        "Feature_Engineer_Agent": "Feature_Engineer_Agent",
        "Model_Architect_Agent": "Model_Architect_Agent",
        "Report_Generator_Agent": "Report_Generator_Agent",
        "Chief_Strategist_Agent": "Chief_Strategist_Agent",
    },
)

# ç­–ç•¥å¸«ç¯€é»å¾Œçš„æ¢ä»¶å¼è·¯ç”±
workflow.add_conditional_edges(
    "Chief_Strategist_Agent",
    router_after_strategy,
    {
        "Data_Analysis_Agent": "Data_Analysis_Agent",
        "Feature_Engineer_Agent": "Feature_Engineer_Agent",
        "Model_Architect_Agent": "Model_Architect_Agent",
        "Report_Generator_Agent": "Report_Generator_Agent",
        END: END,
    },
)

app = workflow.compile()


# --- 6. åŸ·è¡Œå·¥ä½œæµç¨‹ (Run the Workflow) ---


def setup_titanic_dataset():
    logger.debug("æ­£åœ¨æº–å‚™ç¯„ä¾‹è³‡æ–™é›† (éµé”å°¼è™Ÿ)")
    data_dir = os.path.abspath("./kaggle_workspace/data")

    print(f"Deleting existing directory: {data_dir}")
    if os.path.exists(os.path.abspath("./kaggle_workspace")):
        shutil.rmtree(os.path.abspath("./kaggle_workspace"))

    os.makedirs(data_dir, exist_ok=True)
    train_url = (
        "https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv"
    )
    df = pd.read_csv(train_url)
    from sklearn.model_selection import train_test_split

    # First split: separate test set (20% of total data)
    train_val_df, test_df = train_test_split(
        df, test_size=0.2, random_state=42, stratify=df["Survived"]
    )

    # Second split: separate train and validation from remaining 80%
    # This gives us 60% train, 20% validation, 20% test
    train_df, val_df = train_test_split(
        train_val_df, test_size=0.25, random_state=42, stratify=train_val_df["Survived"]
    )

    # Save all three datasets with target variable intact
    train_path = os.path.join(data_dir, "train.csv")
    val_path = os.path.join(data_dir, "validation.csv")
    test_path = os.path.join(data_dir, "test.csv")

    train_df.to_csv(train_path, index=False)
    val_df.to_csv(val_path, index=False)
    test_df.to_csv(test_path, index=False)

    logger.info(f"è¨“ç·´è³‡æ–™å·²å„²å­˜è‡³: {train_path} (æ¨£æœ¬æ•¸: {len(train_df)})")
    logger.info(f"é©—è­‰è³‡æ–™å·²å„²å­˜è‡³: {val_path} (æ¨£æœ¬æ•¸: {len(val_df)})")
    logger.info(f"æ¸¬è©¦è³‡æ–™å·²å„²å­˜è‡³: {test_path} (æ¨£æœ¬æ•¸: {len(test_df)})")
    logger.info(f"ç¸½æ¨£æœ¬æ•¸: {len(df)}")
    return data_dir


def setup_dataset(file_path: str, base_path: str):
    logger.debug("æ­£åœ¨æº–å‚™ç‰¹å®šè³‡æ–™é›†")
    data_dir = os.path.abspath(base_path)
    if os.path.exists(data_dir):
        print(f"Deleting existing directory: {data_dir}")
        shutil.rmtree(data_dir)

    os.makedirs(data_dir, exist_ok=True)
    df = pd.read_csv(file_path)
    # df = df.dropna(subset=["ir"])
    from sklearn.model_selection import train_test_split

    # First split: separate test set (20% of total data)
    train_val_df, test_df = train_test_split(
        df,
        test_size=0.2,
        random_state=42,
        # stratify=df["ir"],
    )

    # Second split: separate train and validation from remaining 80%
    # This gives us 60% train, 20% validation, 20% test
    train_df, val_df = train_test_split(
        train_val_df,
        test_size=0.25,
        random_state=42,
        # stratify=train_val_df["ir"],
    )

    # Save all three datasets with target variable intact
    train_path = os.path.join(data_dir, "train.csv")
    val_path = os.path.join(data_dir, "validation.csv")
    test_path = os.path.join(data_dir, "test.csv")

    train_df.to_csv(train_path, index=False)
    val_df.to_csv(val_path, index=False)
    test_df.to_csv(test_path, index=False)

    logger.info(f"è¨“ç·´è³‡æ–™å·²å„²å­˜è‡³: {train_path} (æ¨£æœ¬æ•¸: {len(train_df)})")
    logger.info(f"é©—è­‰è³‡æ–™å·²å„²å­˜è‡³: {val_path} (æ¨£æœ¬æ•¸: {len(val_df)})")
    logger.info(f"æ¸¬è©¦è³‡æ–™å·²å„²å­˜è‡³: {test_path} (æ¨£æœ¬æ•¸: {len(test_df)})")
    logger.info(f"ç¸½æ¨£æœ¬æ•¸: {len(df)}")
    return data_dir


if __name__ == "__main__":
    USE_TANICS_DATASET = True  # è¨­ç½®ç‚º True ä»¥ä½¿ç”¨éµé”å°¼è™Ÿè³‡æ–™é›†ï¼Œå¦å‰‡ä½¿ç”¨è‡ªå®šç¾©è³‡æ–™é›†

    if USE_TANICS_DATASET:
        data_directory = setup_titanic_dataset()
        problem = "ä½ å¥½ï¼Œä½ çš„ä»»å‹™æ˜¯åˆ†æéµé”å°¼è™Ÿè³‡æ–™é›†ï¼Œé æ¸¬å“ªäº›ä¹˜å®¢èƒ½å¤ ç”Ÿé‚„ã€‚é€™æ˜¯ä¸€å€‹äºŒå…ƒåˆ†é¡å•é¡Œï¼Œè«‹å»ºç«‹ä¸€å€‹æ¨¡å‹ä¸¦ç”¢å‡ºåˆ†æå ±å‘Šã€‚target_column is 'Survived'ã€‚Don't use one-hot encoding."
        workspace_paths = setup_workspace_structure()

    # æƒæåˆå§‹å¯ç”¨æª”æ¡ˆ
    available_files = scan_available_files(workspace_paths)

    # è¨­ç½®é è¨­ç›®æ¨™æ¬„ä½
    default_target = "target"

    initial_state = {
        "problem_statement": problem,
        "data_path": workspace_paths["data"],  # ä½¿ç”¨è¨­ç½®å¥½çš„è³‡æ–™è·¯å¾‘
        "target_column": default_target,  # è¨­ç½®é è¨­ç›®æ¨™æ¬„ä½
        "workspace_paths": workspace_paths,  # æ·»åŠ æ‰€æœ‰è·¯å¾‘è³‡è¨Š
        "available_files": available_files,  # æ·»åŠ åˆå§‹æª”æ¡ˆæ¸…å–®
        "iteration_history": [],
        "error_count": 0,
        "validation_score": 0.0,
        "test_score": 0.0,
        "submission_file_path": "",
        "feature_columns": [],
        "model_performance": {},
        "best_hyperparameters": {},
        "preprocessing_steps": [],
    }

    logger.info("=== å·¥ä½œå€åˆå§‹åŒ–å®Œæˆ ===")
    logger.info("å¯ç”¨è³‡æ–™å¤¾:")
    for folder_name, folder_path in workspace_paths.items():
        logger.info(f"  {folder_name}: {folder_path}")
    logger.info("é–‹å§‹åŸ·è¡Œ Kaggle å·¥ä½œæµç¨‹...")

    final_state = None
    # å¢åŠ  recursion_limit ä»¥æ‡‰å°å¯èƒ½çš„é‡è©¦
    try:
        for s in app.stream(initial_state, {"recursion_limit": 60}):
            logger.debug("---")
            node_name = list(s.keys())[0]
            logger.debug(f"ç¯€é»: {node_name}")
            logger.debug(f"{s[node_name]}")
            final_state = s[node_name]
    except Exception as e:
        import traceback

        print(traceback.format_exc())
        logger.error(f"å·¥ä½œæµç¨‹åŸ·è¡Œå¤±æ•—: {e}")
        logger.info("===Reach recursion_limit 75===")

    # è¨˜éŒ„æœ€çµ‚çµæœå’ŒéŒ¯èª¤çµ±è¨ˆ
    log_error_stats()
    logger.info("=== å·¥ä½œæµç¨‹åŸ·è¡Œå®Œç•¢ ===")

    if final_state is not None:
        logger.info(f"æœ€çµ‚é©—è­‰åˆ†æ•¸: {final_state.get('validation_score')}")
        logger.info(f"æœ€çµ‚æ¸¬è©¦åˆ†æ•¸: {final_state.get('test_score')}")
        logger.info(f"æäº¤æª”æ¡ˆè·¯å¾‘: {final_state.get('submission_file_path')}")
        logger.info("è¿­ä»£æ­·å²:")
        for item in final_state.get("iteration_history", []):
            logger.info(f"- {item}")
    else:
        logger.warning("å·¥ä½œæµç¨‹åŸ·è¡Œæœªèƒ½å®Œæˆï¼Œæ²’æœ‰æœ€çµ‚ç‹€æ…‹è³‡è¨Š")
